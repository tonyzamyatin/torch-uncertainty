# lightning.pytorch==2.1.3
seed_everything: false
eval_after_fit: true
trainer:
  fast_dev_run: false
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  max_epochs: 75
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: logs/lenet
      name: standard
      default_hp_metric: false
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      monitor: val/cls/Acc
      mode: max
      save_last: true
  - class_path: lightning.pytorch.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step
  - class_path: lightning.pytorch.callbacks.EarlyStopping
    init_args:
      monitor: val/cls/Acc
      mode: max
      min_delta: 0.001
      patience: 10
      check_finite: true
model:
  # ClassificationRoutine
  model:
    # MlP
    class_path: torch_uncertainty.models.mlp.mlp
    init_args:
      in_features: 784 # 28*28
      num_outputs: 10
      hidden_dims: [64, 32, 16]
      activation: torch.nn.ReLU
      dropout_rate: 0.05
  num_classes: 10
  loss: CrossEntropyLoss
data:
  root: ./data
  batch_size: 128
  num_workers: 127
  eval_ood: true
  eval_shift: true
optimizer:
  # SGD with momentum
  # class_path: torch.optim.SGD
  # init_args:
  #   lr: 0.05
  #   momentum: 0.9
  #   weight_decay: 5e-4
  #   nesterov: true
  class_path: torch.optim.Adam
  init_args:
    lr: 0.001
    weight_decay: 1e-4
    betas: [0.9, 0.999]
    eps: 1e-08
lr_scheduler:
  class_path: torch.optim.lr_scheduler.MultiStepLR
  init_args:
    milestones:
    - 25
    - 50
    gamma: 0.1
